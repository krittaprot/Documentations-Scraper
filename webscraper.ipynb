{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Documentations Scraper\n",
    "Author: Krittaprot Tangkittikun\n",
    "Date: Mar 19, 2024\n",
    "Purpose: The notebook includes the code for loading HTML source code using selenium.\n",
    "The page content is then parsed using beautifulsoup for a structured string/text format\n",
    "for further use in LLM fine-tuning datasets generation.\n",
    "\n",
    "Environment Requirements:\n",
    "pip install beautifulsoup4\n",
    "pip install selenium\n",
    "'''\n",
    "\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def load_content(link):\n",
    "    # Create a WebDriver instance\n",
    "    driver = webdriver.Chrome()\n",
    "\n",
    "    # Open the webpage\n",
    "    driver.get(link)\n",
    "\n",
    "    # Get the page source\n",
    "    page_source = driver.page_source\n",
    "\n",
    "    # Close the browser\n",
    "    driver.quit()\n",
    "    return page_source\n",
    "\n",
    "def get_content_with_hyperlinks(soup):\n",
    "    content_parts = []  # List to hold all parts of the content\n",
    "    \n",
    "    current_h3 = None\n",
    "    current_h4 = None\n",
    "    \n",
    "    for descendant in soup.descendants:\n",
    "        if descendant.name == 'h3':\n",
    "            current_h3 = descendant.get_text(strip=True)\n",
    "            current_h4 = None  # Reset H4 because we're in a new H3 section\n",
    "            content_parts.append(f'\\n\\n{current_h3}\\n')  # Add two newlines before H3, one after\n",
    "        elif descendant.name == 'h4':\n",
    "            current_h4 = descendant.get_text(strip=True)\n",
    "            content_parts.append(f'\\n  {current_h4}\\n')  # Newline before and after H4, with indentation\n",
    "        elif descendant.name == 'p':\n",
    "            paragraph_with_links = \"\"\n",
    "            for content in descendant.contents:\n",
    "                if content.name == 'a':\n",
    "                    link_text = content.get_text(strip=True)\n",
    "                    link_url = content['href']\n",
    "                    # Add space before link if paragraph doesn't end with space\n",
    "                    if paragraph_with_links and not paragraph_with_links.endswith(' '):\n",
    "                        paragraph_with_links += ' '\n",
    "                    paragraph_with_links += f'{link_text} ({link_url}) '\n",
    "                else:\n",
    "                    paragraph_with_links += str(content)\n",
    "            \n",
    "            # Clean up the paragraph text to remove extra spaces\n",
    "            paragraph_with_links = ' '.join(paragraph_with_links.split())\n",
    "            \n",
    "            # Append paragraph text with appropriate indentation\n",
    "            if current_h4:  # If inside an H4 section\n",
    "                content_parts.append(f'    {paragraph_with_links}\\n')\n",
    "            elif current_h3:  # Directly under an H3 section\n",
    "                content_parts.append(f'  {paragraph_with_links}\\n')\n",
    "\n",
    "    # Join all parts into a single string\n",
    "    full_content = ''.join(content_parts).strip()  # Strip leading/trailing whitespace\n",
    "    return full_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Web content saved to scraped_data/0_competitions.txt\n",
      "Web content saved to scraped_data/1_datasets.txt\n",
      "Web content saved to scraped_data/2_notebooks.txt\n",
      "Web content saved to scraped_data/3_api.txt\n",
      "Error: 'web_content' is not defined or empty.\n",
      "Web content saved to scraped_data/5_tpu.txt\n",
      "Web content saved to scraped_data/6_models.txt\n",
      "Web content saved to scraped_data/7_competitions-setup.txt\n",
      "Web content saved to scraped_data/8_organizations.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Ensure the directory exists, create if it doesn't\n",
    "directory = os.path.dirname(file_path)\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "links = ['https://www.kaggle.com/docs/competitions', \n",
    "         'https://www.kaggle.com/docs/datasets',\n",
    "         'https://www.kaggle.com/docs/notebooks',\n",
    "         'https://www.kaggle.com/docs/api',\n",
    "         'https://www.kaggle.com/docs/efficient-gpu-usage',\n",
    "         'https://www.kaggle.com/docs/tpu',\n",
    "         'https://www.kaggle.com/docs/models',\n",
    "         'https://www.kaggle.com/docs/competitions-setup',\n",
    "         'https://www.kaggle.com/docs/organizations'\n",
    "         ]\n",
    "\n",
    "for i, link in enumerate(links):\n",
    "    pagename = link.split('/')[-1]\n",
    "    page_source = load_content(link)\n",
    "    soup = BeautifulSoup(page_source)\n",
    "    web_content = get_content_with_hyperlinks(soup)\n",
    "    file_path = f'scraped_data/{i}_{pagename}.txt'\n",
    "\n",
    "    # Check if web_content is defined and non-empty\n",
    "    if web_content:\n",
    "        # Open the file in write mode and write the string data to it\n",
    "        with open(file_path, 'w') as file:\n",
    "            file.write(web_content)\n",
    "        print(\"Web content saved to\", file_path)\n",
    "    else:\n",
    "        print(\"Error: 'web_content' is not defined or empty.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resources: \n",
    "1) [How to SCRAPE DYNAMIC websites with Selenium](https://www.youtube.com/watch?v=lTypMlVBFM4)\n",
    "2) [Helium - high level webscraper (Selenium-based)](https://github.com/mherrmann/helium?tab=readme-ov-file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "webscraper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
